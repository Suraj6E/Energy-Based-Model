{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Image Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display images\n",
    "def show_image(img, title=\"Image\", cmap=None):\n",
    "    plt.imshow(img, cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction using ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(image_path):\n",
    "    # Load a pretrained ResNet model\n",
    "    model = resnet50(pretrained=True)\n",
    "    model.eval()\n",
    "\n",
    "    # Define a transform to preprocess the input image\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path)\n",
    "    input_tensor = preprocess(image)\n",
    "    input_batch = input_tensor.unsqueeze(0)  # Create a mini-batch as expected by the model\n",
    "\n",
    "    # Forward pass through the model\n",
    "    with torch.no_grad():\n",
    "        features = model(input_batch)\n",
    "\n",
    "    # Extract features from the last fully connected layer\n",
    "    feature_vector = features.squeeze().numpy()\n",
    "    print(\"Feature vector shape:\", feature_vector.shape)\n",
    "\n",
    "    # Visualize the features (projected back onto the image for simplicity)\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    keypoints = [cv2.KeyPoint(x=int(feature_vector[i] % img.shape[1]), y=int(feature_vector[i] // img.shape[1]), _size=10) for i in range(min(50, len(feature_vector)))]\n",
    "    img_with_features = cv2.drawKeypoints(img, keypoints, None, color=(0, 255, 0), flags=cv2.DrawMatchesFlags_DRAW_RICH_KEYPOINTS)\n",
    "    \n",
    "    return img_with_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge Detection using Sobel Operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_detection(image_path):\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert('L')  # Convert to grayscale\n",
    "    preprocess = transforms.ToTensor()\n",
    "    input_tensor = preprocess(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Define the Sobel operator kernels\n",
    "    sobel_x = torch.tensor([[-1., 0., 1.],\n",
    "                            [-2., 0., 2.],\n",
    "                            [-1., 0., 1.]]).unsqueeze(0).unsqueeze(0)\n",
    "    sobel_y = torch.tensor([[-1., -2., -1.],\n",
    "                            [0., 0., 0.],\n",
    "                            [1., 2., 1.]]).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # Apply Sobel operators\n",
    "    grad_x = F.conv2d(input_tensor, sobel_x, padding=1)\n",
    "    grad_y = F.conv2d(input_tensor, sobel_y, padding=1)\n",
    "\n",
    "    # Calculate the gradient magnitude\n",
    "    grad_magnitude = torch.sqrt(grad_x**2 + grad_y**2).squeeze().numpy()\n",
    "\n",
    "    return grad_magnitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure from Motion (SfM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_from_motion(image_path1, image_path2):\n",
    "    # Load images\n",
    "    img1 = cv2.imread(image_path1, cv2.IMREAD_GRAYSCALE)\n",
    "    img2 = cv2.imread(image_path2, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # Detect ORB keypoints and descriptors\n",
    "    orb = cv2.ORB_create()\n",
    "    keypoints1, descriptors1 = orb.detectAndCompute(img1, None)\n",
    "    keypoints2, descriptors2 = orb.detectAndCompute(img2, None)\n",
    "\n",
    "    # Match descriptors using BFMatcher\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    matches = bf.match(descriptors1, descriptors2)\n",
    "    matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "    # Draw matches\n",
    "    img_matches = cv2.drawMatches(img1, keypoints1, img2, keypoints2, matches[:50], None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "    \n",
    "    # Convert keypoints to points\n",
    "    points1 = np.float32([keypoints1[m.queryIdx].pt for m in matches])\n",
    "    points2 = np.float32([keypoints2[m.trainIdx].pt for m in matches])\n",
    "\n",
    "    # Compute the essential matrix\n",
    "    E, mask = cv2.findEssentialMat(points1, points2, focal=1.0, pp=(0, 0), method=cv2.RANSAC, prob=0.999, threshold=1.0)\n",
    "    _, R, t, mask = cv2.recoverPose(E, points1, points2)\n",
    "\n",
    "    # Assuming K is the camera intrinsic matrix\n",
    "    K = np.array([[1, 0, 0],\n",
    "                  [0, 1, 0],\n",
    "                  [0, 0, 1]], dtype=float)\n",
    "\n",
    "    # Projection matrices\n",
    "    P1 = np.dot(K, np.hstack((np.eye(3), np.zeros((3, 1)))))\n",
    "    P2 = np.dot(K, np.hstack((R, t)))\n",
    "\n",
    "    # Triangulate points\n",
    "    points1 = points1[mask.ravel() == 1]\n",
    "    points2 = points2[mask.ravel() == 1]\n",
    "    points4D = cv2.triangulatePoints(P1, P2, points1.T, points2.T)\n",
    "    points3D = points4D[:3] / points4D[3]\n",
    "\n",
    "    return img_matches, points3D.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-View Stereo (MVS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDepthNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleDepthNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 1, kernel_size=3, stride=1, padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "        return x\n",
    "\n",
    "def dense_reconstruction(image_path):\n",
    "    # Load and preprocess image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    input_tensor = preprocess(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Initialize and run the network\n",
    "    model = SimpleDepthNet()\n",
    "    output = model(input_tensor)\n",
    "    depth_map = output.squeeze().detach().numpy()\n",
    "\n",
    "    return depth_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature vector shape: (1000,)\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) :-1: error: (-5:Bad argument) in function 'KeyPoint'\n> Overload resolution failed:\n>  - KeyPoint() missing required argument 'size' (pos 3)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m image_path2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./productImages/pepsi.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Step 1: Feature Extraction\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_extraction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m show_image(image, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature Extraction Result\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Step 2: Edge Detection\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 30\u001b[0m, in \u001b[0;36mfeature_extraction\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m     28\u001b[0m img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(image_path)\n\u001b[0;32m     29\u001b[0m img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(img, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m---> 30\u001b[0m keypoints \u001b[38;5;241m=\u001b[39m [\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mKeyPoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfeature_vector\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfeature_vector\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m50\u001b[39m, \u001b[38;5;28mlen\u001b[39m(feature_vector)))]\n\u001b[0;32m     31\u001b[0m img_with_features \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mdrawKeypoints(img, keypoints, \u001b[38;5;28;01mNone\u001b[39;00m, color\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m), flags\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mDrawMatchesFlags_DRAW_RICH_KEYPOINTS)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img_with_features\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.9.0) :-1: error: (-5:Bad argument) in function 'KeyPoint'\n> Overload resolution failed:\n>  - KeyPoint() missing required argument 'size' (pos 3)\n"
     ]
    }
   ],
   "source": [
    "image_path1 = './productImages/pepsi.jpg'\n",
    "image_path2 = './productImages/pepsi.jpg'\n",
    "\n",
    "# Step 1: Feature Extraction\n",
    "image = feature_extraction(image_path1)\n",
    "show_image(image, \"Feature Extraction Result\")\n",
    "\n",
    "# Step 2: Edge Detection\n",
    "edges = edge_detection(image_path1)\n",
    "show_image(edges, \"Edge Detection Result\", cmap='gray')\n",
    "\n",
    "# Step 3: Structure from Motion\n",
    "img_matches, points3D = structure_from_motion(image_path1, image_path2)\n",
    "show_image(img_matches, \"Feature Matching Result\")\n",
    "print(\"3D Points:\\n\", points3D)\n",
    "\n",
    "# Step 4: Multi-View Stereo\n",
    "depth_map = dense_reconstruction(image_path1)\n",
    "show_image(depth_map, \"Dense Reconstruction Result\", cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
