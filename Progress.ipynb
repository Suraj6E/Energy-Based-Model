{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Image Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display images\n",
    "def show_image(img, title=\"Image\", cmap=None):\n",
    "    plt.imshow(img, cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction using ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(image_path):\n",
    "    # Load a pretrained ResNet model\n",
    "    model = resnet50(pretrained=True)\n",
    "    model.eval()\n",
    "\n",
    "    # Define a transform to preprocess the input image\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path)\n",
    "    input_tensor = preprocess(image)\n",
    "    input_batch = input_tensor.unsqueeze(0)  # Create a mini-batch as expected by the model\n",
    "\n",
    "    # Forward pass through the model\n",
    "    with torch.no_grad():\n",
    "        features = model(input_batch)\n",
    "\n",
    "    # Extract features from the last fully connected layer\n",
    "    feature_vector = features.squeeze().numpy()\n",
    "    print(\"Feature vector shape:\", feature_vector.shape)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge Detection using Sobel Operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_detection(image_path):\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert('L')  # Convert to grayscale\n",
    "    preprocess = transforms.ToTensor()\n",
    "    input_tensor = preprocess(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Define the Sobel operator kernels\n",
    "    sobel_x = torch.tensor([[-1., 0., 1.],\n",
    "                            [-2., 0., 2.],\n",
    "                            [-1., 0., 1.]]).unsqueeze(0).unsqueeze(0)\n",
    "    sobel_y = torch.tensor([[-1., -2., -1.],\n",
    "                            [0., 0., 0.],\n",
    "                            [1., 2., 1.]]).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # Apply Sobel operators\n",
    "    grad_x = F.conv2d(input_tensor, sobel_x, padding=1)\n",
    "    grad_y = F.conv2d(input_tensor, sobel_y, padding=1)\n",
    "\n",
    "    # Calculate the gradient magnitude\n",
    "    grad_magnitude = torch.sqrt(grad_x**2 + grad_y**2).squeeze().numpy()\n",
    "\n",
    "    return grad_magnitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure from Motion (SfM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_from_motion(image_path1, image_path2):\n",
    "    # Load images\n",
    "    img1 = cv2.imread(image_path1, cv2.IMREAD_GRAYSCALE)\n",
    "    img2 = cv2.imread(image_path2, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # Detect ORB keypoints and descriptors\n",
    "    orb = cv2.ORB_create()\n",
    "    keypoints1, descriptors1 = orb.detectAndCompute(img1, None)\n",
    "    keypoints2, descriptors2 = orb.detectAndCompute(img2, None)\n",
    "\n",
    "    # Match descriptors using BFMatcher\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    matches = bf.match(descriptors1, descriptors2)\n",
    "    matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "    # Draw matches\n",
    "    img_matches = cv2.drawMatches(img1, keypoints1, img2, keypoints2, matches[:50], None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "    \n",
    "    # Convert keypoints to points\n",
    "    points1 = np.float32([keypoints1[m.queryIdx].pt for m in matches])\n",
    "    points2 = np.float32([keypoints2[m.trainIdx].pt for m in matches])\n",
    "\n",
    "    # Compute the essential matrix\n",
    "    E, mask = cv2.findEssentialMat(points1, points2, focal=1.0, pp=(0, 0), method=cv2.RANSAC, prob=0.999, threshold=1.0)\n",
    "    _, R, t, mask = cv2.recoverPose(E, points1, points2)\n",
    "\n",
    "    # Assuming K is the camera intrinsic matrix\n",
    "    K = np.array([[1, 0, 0],\n",
    "                  [0, 1, 0],\n",
    "                  [0, 0, 1]], dtype=float)\n",
    "\n",
    "    # Projection matrices\n",
    "    P1 = np.hstack((np.eye(3), np.zeros((3, 1))))\n",
    "    P2 = np.hstack((R, t))\n",
    "\n",
    "    # Triangulate points\n",
    "    points1 = points1[mask.ravel() == 1]\n",
    "    points2 = points2[mask.ravel() == 1]\n",
    "    points4D = cv2.triangulatePoints(P1, P2, points1.T, points2.T)\n",
    "    points3D = points4D[:3] / points4D[3]\n",
    "\n",
    "    return img_matches, points3D.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-View Stereo (MVS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDepthNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleDepthNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 1, kernel_size=3, stride=1, padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "        return x\n",
    "\n",
    "def dense_reconstruction(image_path):\n",
    "    # Load and preprocess image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    input_tensor = preprocess(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Initialize and run the network\n",
    "    model = SimpleDepthNet()\n",
    "    output = model(input_tensor)\n",
    "    depth_map = output.squeeze().detach().numpy()\n",
    "\n",
    "    return depth_map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
